<!DOCTYPE html>
<html lang="en">
<head>
    <title>Thanh Bao TRAN - Portfolio</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Robotics Engineer | Control Theory | Autonomous Systems">
    <meta name="author" content="Thanh Bao TRAN">

    <!-- Bootstrap core CSS -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts -->
    <link href="https://use.fontawesome.com/releases/v5.15.1/css/all.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">

    <style>
      body {
        overflow-x: hidden;
        font-family: 'Roboto Slab', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      p {
        line-height: 1.75;
      }

      a {
        color: #fed136;
      }

      a:hover {
        color: #fec503;
      }

      .text-primary {
        color: #fed136 !important;
      }

      h1, h2, h3, h4, h5, h6 {
        font-weight: 700;
        font-family: 'Montserrat', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      section {
        padding: 100px 0;
      }

      section h2.section-heading {
        font-size: 40px;
        margin-top: 0;
        margin-bottom: 15px;
      }

      section h3.section-subheading {
        font-size: 16px;
        font-weight: 400;
        font-style: italic;
        margin-bottom: 75px;
        text-transform: none;
        font-family: 'Droid Serif', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      .btn {
        font-family: 'Montserrat', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
        font-weight: 700;
      }

      .btn-xl {
        font-size: 18px;
        padding: 20px 40px;
      }

      .btn-primary {
        background-color: #fed136;
        border-color: #fed136;
      }

      .btn-primary:active, .btn-primary:focus, .btn-primary:hover {
        background-color: #fec810 !important;
        border-color: #fec810 !important;
        color: white;
      }

      /* Navigation */
      #mainNav {
        background-color: #212529;
      }

      #mainNav .navbar-toggler {
        font-size: 12px;
        right: 0;
        padding: 13px;
        text-transform: uppercase;
        color: white;
        border: 0;
        background-color: #fed136;
        font-family: 'Montserrat', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      #mainNav .navbar-brand {
        color: #fed136;
        font-family: 'Montserrat', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
        font-weight: 700;
        letter-spacing: 0.0625em;
        text-transform: uppercase;
      }

      #mainNav .navbar-brand:hover {
        color: #fec503;
      }

      #mainNav .navbar-nav .nav-item .nav-link {
        font-size: 90%;
        font-weight: 400;
        padding: 0.75em 0;
        letter-spacing: 1px;
        color: white;
        font-family: 'Montserrat', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      #mainNav .navbar-nav .nav-item .nav-link.active, #mainNav .navbar-nav .nav-item .nav-link:hover {
        color: #fed136;
      }

      @media (min-width: 992px) {
        #mainNav {
          padding-top: 25px;
          padding-bottom: 25px;
          transition: padding-top 0.3s, padding-bottom 0.3s;
          border: none;
          background-color: transparent;
        }

        #mainNav .navbar-brand {
          font-size: 1.75em;
          transition: all 0.3s;
        }

        #mainNav .navbar-nav .nav-item .nav-link {
          padding: 1.1em 1em !important;
        }

        #mainNav.navbar-shrink {
          padding-top: 0;
          padding-bottom: 0;
          background-color: #212529;
        }

        #mainNav.navbar-shrink .navbar-brand {
          font-size: 1.25em;
          padding: 12px 0;
        }
      }

      /* Header */
      header.masthead {
        text-align: center;
        color: white;
        background-image: url('hero-background-optimized.jpg');
        background-repeat: no-repeat;
        background-attachment: scroll;
        background-position: center center;
        background-size: cover;
      }

      header.masthead .intro-text {
        padding-top: 150px;
        padding-bottom: 100px;
      }

      header.masthead .intro-text .intro-lead-in {
        font-size: 22px;
        font-style: italic;
        line-height: 22px;
        margin-bottom: 25px;
        font-family: 'Droid Serif', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      header.masthead .intro-text .intro-heading {
        font-size: 50px;
        font-weight: 700;
        line-height: 50px;
        margin-bottom: 25px;
        font-family: 'Montserrat', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      @media (min-width: 768px) {
        header.masthead .intro-text {
          padding-top: 300px;
          padding-bottom: 200px;
        }

        header.masthead .intro-text .intro-lead-in {
          font-size: 40px;
          font-style: italic;
          line-height: 40px;
          margin-bottom: 25px;
          font-family: 'Droid Serif', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
        }

        header.masthead .intro-text .intro-heading {
          font-size: 75px;
          font-weight: 700;
          line-height: 75px;
          margin-bottom: 50px;
          font-family: 'Montserrat', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
        }
      }

      /* Portfolio */
      #portfolio .portfolio-item {
        right: 0;
        margin: 0 0 15px;
      }

      #portfolio .portfolio-item .portfolio-link {
        position: relative;
        display: block;
        max-width: 400px;
        margin: 0 auto;
        cursor: pointer;
      }

      #portfolio .portfolio-item .portfolio-link .portfolio-hover {
        position: absolute;
        width: 100%;
        height: 100%;
        transition: all ease 0.5s;
        opacity: 0;
        background: rgba(254, 209, 54, 0.9);
      }

      #portfolio .portfolio-item .portfolio-link .portfolio-hover:hover {
        opacity: 1;
      }

      #portfolio .portfolio-item .portfolio-link .portfolio-hover .portfolio-hover-content {
        font-size: 20px;
        position: absolute;
        top: 50%;
        width: 100%;
        height: 20px;
        margin-top: -12px;
        text-align: center;
        color: white;
      }

      #portfolio .portfolio-item .portfolio-link .portfolio-hover .portfolio-hover-content i {
        margin-top: -12px;
      }

      #portfolio .portfolio-item .portfolio-link .portfolio-hover .portfolio-hover-content h3,
      #portfolio .portfolio-item .portfolio-link .portfolio-hover .portfolio-hover-content h4 {
        margin: 0;
      }

      #portfolio .portfolio-item .portfolio-caption {
        max-width: 400px;
        margin: 0 auto;
        padding: 25px;
        text-align: center;
        background-color: #fff;
      }

      #portfolio .portfolio-item .portfolio-caption h4 {
        margin: 0;
        text-transform: none;
      }

      #portfolio .portfolio-item .portfolio-caption p {
        font-size: 16px;
        font-style: italic;
        margin: 0;
        font-family: 'Droid Serif', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      #portfolio * {
        z-index: 2;
      }

      /* Portfolio Modal */
      .portfolio-modal .modal-dialog {
        margin: 1rem;
        max-width: 100vw;
      }

      .portfolio-modal .modal-content {
        padding: 100px 0;
        text-align: center;
      }

      .portfolio-modal .modal-content h2, .portfolio-modal .modal-content .h2 {
        font-size: 3em;
        margin-bottom: 15px;
      }

      .portfolio-modal .modal-content p {
        margin-bottom: 30px;
      }

      .portfolio-modal .modal-content p.item-intro {
        font-size: 16px;
        font-style: italic;
        margin: 20px 0 30px;
        font-family: 'Droid Serif', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
      }

      .portfolio-modal .modal-content ul.list-inline {
        margin-top: 0;
        margin-bottom: 30px;
      }

      .portfolio-modal .modal-content img {
        margin-bottom: 30px;
      }

      .portfolio-modal .modal-content button {
        cursor: pointer;
      }

      .portfolio-modal .close-modal {
        position: absolute;
        top: 25px;
        right: 25px;
        width: 75px;
        height: 75px;
        cursor: pointer;
        background-color: transparent;
      }

      .portfolio-modal .close-modal:hover {
        opacity: 0.3;
      }

      .portfolio-modal .close-modal .lr {
        z-index: 1051;
        width: 1px;
        height: 75px;
        margin-left: 35px;
        background-color: #212529;
        transform: rotate(45deg);
      }

      .portfolio-modal .close-modal .lr .rl {
        z-index: 1052;
        width: 1px;
        height: 75px;
        background-color: #212529;
        transform: rotate(90deg);
      }

      @media (min-width: 768px) {
        .portfolio-modal .modal-dialog {
          margin: 1.75rem auto;
          max-width: 900px;
        }

        .portfolio-modal .modal-content h2, .portfolio-modal .modal-content .h2 {
          font-size: 4.5em;
        }
      }

      /* Tags */
      .boxed {
        border: 1px solid #fed136;
        padding: 5px 10px;
        margin: 2px;
        border-radius: 3px;
        display: inline-block;
        background-color: #fff9e6;
      }

      /* Team */
      .team-member {
        margin-bottom: 50px;
        text-align: center;
      }

      .team-member img {
        width: 225px;
        height: 225px;
        border: 7px solid rgba(0, 0, 0, 0.1);
      }

      .team-member h4 {
        margin-top: 25px;
        margin-bottom: 0;
        text-transform: none;
      }

      .team-member p {
        margin-top: 0;
      }

      ul.social-buttons {
        margin-bottom: 0;
      }

      ul.social-buttons li a {
        font-size: 20px;
        line-height: 40px;
        display: block;
        width: 40px;
        height: 40px;
        transition: all 0.3s;
        color: white;
        border-radius: 100%;
        outline: none;
        background-color: #212529;
      }

      ul.social-buttons li a:active, ul.social-buttons li a:focus, ul.social-buttons li a:hover {
        background-color: #fed136;
      }
    </style>
</head>

<body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Thanh Bao TRAN</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#TechnicalSkills">Skills</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Contact Me</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container">
        <div class="intro-text">
          <div class="intro-heading text-uppercase" style="background-color: rgba(26, 26, 26, 0.7); color:rgba(253, 201, 43, 1); border-radius:5px;">Thanh Bao TRAN</div>
          <div class="intro-lead-in">Robotics Engineer | Control Theory | Autonomous Systems</div>
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="https://drive.google.com/file/d/10XS1kC_fWfaSK0QpQML5fXv6fsZ4Etbo/view?usp=sharing" target="_blank">Resume</a>
        </div>
      </div>
    </header>

    <!-- Portfolio Grid -->
    <section class="bg-light" id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Projects</h2>
            <h3 class="section-subheading text-muted">September 2023 ~ Now</h3>
          </div>
        </div>
        <div class="row">

          <!-- Project 1: UAV Tension Estimation -->
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#tensionestimation">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="assets/uav/figure8/3DTrajectories.png" alt="UAV Tension Estimation">
            </a>
            <div class="portfolio-caption">
              <h4>Tension Estimation UAV-Slung Load</h4>
              <p class="text-muted">EKF | ADRC | Learning MPC<br>Real-time tension estimation for safe UAV operations</p>
            </div>
          </div>

          <!-- Project 2: Dual UR5 Teleoperation -->
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#handteleoperation">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="assets/hand_tracking/cover.png" alt="Hand Teleoperation">
            </a>
            <div class="portfolio-caption">
              <h4>Dual UR5 Hand Teleoperation</h4>
              <p class="text-muted">ROS2 | MediaPipe | Computer Vision<br>Intuitive hand gesture control for dual-arm manipulation</p>
            </div>
          </div>

          <!-- Project 3: Follow Me Robot -->
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#followme">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="assets/followme/cover.png" alt="Follow Me Robot">
            </a>
            <div class="portfolio-caption">
              <h4>Follow Me Robot</h4>
              <p class="text-muted">Laser Detection | Kalman Filter | ROS2<br>Autonomous human-following mobile robot</p>
            </div>
          </div>

          <!-- Project 4: Dual Franka DRL -->
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#dualfranka">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="assets/dual_franka/cover.png" alt="Dual Franka DRL">
            </a>
            <div class="portfolio-caption">
              <h4>Dual Franka Arm DRL</h4>
              <p class="text-muted">Deep RL | Isaac Gym | PPO<br>Coordinated manipulation with reinforcement learning</p>
            </div>
          </div>

          <!-- Project 5: Robothon TUM -->
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#robothon">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="https://via.placeholder.com/400x300.png?text=Robothon+TUM" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Robothon TUM</h4>
              <p class="text-muted">Robot Control | Gazebo | CoppeliaSim<br>Advanced robot control and dynamics</p>
            </div>
          </div>

          <!-- Project 6: ZV Shaper ADRC -->
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#zvadrc">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="https://via.placeholder.com/400x300.png?text=Crane+Control" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>ZV Shaper-ADRC Crane Control</h4>
              <p class="text-muted">ADRC | Input Shaping | Control Theory<br>Published research on crane control</p>
            </div>
          </div>

        </div>
      </div>
    </section>

    <!-- Technical Skills -->
    <section id="TechnicalSkills">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Skills</h2>
          </div>
        </div>
        <div class="row text-center">
          <div class="col-md-4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-robot fa-stack-1x fa-inverse"></i>
            </span>
            <h4 class="service-heading">Control Theory</h4>
            <p class="text-muted">Linear & Nonlinear Control, MPC<br>ADRC, Optimal Control<br>Multi-Agent Systems</p>
          </div>
          <div class="col-md-4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-cogs fa-stack-1x fa-inverse"></i>
            </span>
            <h4 class="service-heading">Robotics</h4>
            <p class="text-muted">ROS2, Gazebo, RViz<br>Isaac Sim/Gym<br>Motion Planning, SLAM<br>Computer Vision</p>
          </div>
          <div class="col-md-4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-code fa-stack-1x fa-inverse"></i>
            </span>
            <h4 class="service-heading">Programming</h4>
            <p class="text-muted">Python, C++, MATLAB<br>PyTorch, OpenCV<br>Git, Docker, Linux</p>
          </div>
          <div class="col-md-4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-brain fa-stack-1x fa-inverse"></i>
            </span>
            <h4 class="service-heading">AI & Machine Learning</h4>
            <p class="text-muted">Deep Reinforcement Learning<br>Computer Vision<br>Neural Networks</p>
          </div>
          <div class="col-md-4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-tools fa-stack-1x fa-inverse"></i>
            </span>
            <h4 class="service-heading">Tools & Libraries</h4>
            <p class="text-muted">Eigen, Sophus, g2o, Ceres<br>scikit-learn<br>MATLAB/Simulink, Blender</p>
          </div>
          <div class="col-md-4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-flask fa-stack-1x fa-inverse"></i>
            </span>
            <h4 class="service-heading">Research Interests</h4>
            <p class="text-muted">Autonomous Systems<br>Visual SLAM<br>UAV Control<br>Vision-Language-Action</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Contact -->
    <section class="bg-light" id="team">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Contact me</h2>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-12">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="https://via.placeholder.com/225x225.png?text=TB" alt="">
              <h4>Thanh Bao TRAN</h4>
              <p class="text-muted">Master of Science in Mobile, Autonomous and Robotic Systems<br>@ Grenoble INP - UGA</p>
              <ul class="list-inline social-buttons">
                <li class="list-inline-item">
                  <a href="https://github.com/Tranthanhbao198" target="_blank">
                    <i class="fab fa-github"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="mailto:thanh-bao.tran@grenoble-inp.org">
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="https://www.linkedin.com/in/thanhbaotran" target="_blank">
                    <i class="fab fa-linkedin-in"></i>
                  </a>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Portfolio Modals -->

    <!-- Modal 1: Tension Estimation -->
    <div class="portfolio-modal modal fade" id="tensionestimation" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <h3 class="text-uppercase">Tension Estimation with UAV and Slung Load</h3>
                  <a class="boxed">Extended Kalman Filter</a> <a class="boxed">ADRC</a> <a class="boxed">Learning MPC</a> <a class="boxed">UAV Control</a>
                  <p></p>

                  <p><strong>Duration:</strong> September 2025 — February 2026</p>
                  <p><strong>Location:</strong> GIPSA Lab, CNRS, Grenoble, France</p>
                  <p></p>

                  <h4>Brief Description</h4>
                  <p>Hardware-ready control framework for quadrotor UAVs transporting suspended payloads under realistic sensor noise. Integrates differential flatness planning, Extended Kalman Filter (EKF) sensor fusion, and Active Disturbance Rejection Control (ADRC) for robust trajectory tracking using only noisy IMU and cable angle measurements.</p>
                  <p></p>

                  <h5>Figure-8 Trajectory Demo</h5>
                  <video class="img-fluid d-block mx-auto" width="600" controls>
                    <source src="assets/uav/figure8/animation.webm" type="video/webm">
                    Your browser does not support the video tag.
                  </video>
                  <p class="text-center"><em>Figure-8 (Lissajous) trajectory with ADRC controller</em></p>
                  <p></p>

                  <h4>Motivation</h4>
                  <p>Quadrotor UAVs with cable-suspended payloads face challenges including underactuated cable dynamics, time-varying tension forces, and parameter uncertainties. Real-time tension estimation is critical for safe operations, but traditional model-based methods require accurate models and struggle with sensor noise. This project develops a hardware-ready control and estimation framework that operates solely on noisy sensor measurements, mimicking real deployment conditions.</p>
                  <p></p>

                  <h4>Approach</h4>

                  <h5>Known-Unknown Decomposition</h5>
                  <p>Key innovation: decompose cable force into <strong>known</strong> nominal tension (from differential flatness) and <strong>unknown</strong> disturbances (estimated by Extended State Observer). This prevents naive ADRC from fighting against required tension, ensuring stability while rejecting model mismatch and external forces.</p>

                  <h5>EKF-Based State Estimation</h5>
                  <p>10-dimensional Extended Kalman Filter fuses noisy IMU position measurements (σ<sub>p</sub> = 30mm) and cable angle sensors (σ<sub>θ</sub> = 0.5°) to estimate:</p>
                  <ul>
                    <li>Quadrotor position and velocity</li>
                    <li>Cable tension (primary target)</li>
                    <li>Cable direction vector</li>
                  </ul>
                  <p>EKF estimates replace unavailable true state in the control loop, providing realistic hardware-ready validation.</p>

                  <h5>ESO-Augmented ADRC Controller</h5>
                  <p>Extended State Observer (ESO) estimates total disturbance (model errors, external forces, parameter uncertainties) and cancels it via feedback. Controller operates solely on noisy EKF state estimates—never accesses ground truth.</p>
                  <p></p>

                  <img class="img-fluid d-block mx-auto" src="assets/uav/figure8/3DTrajectories.png" alt="3D Trajectory Tracking">
                  <p class="text-center"><em>3D trajectory tracking: desired (blue) vs actual (red)</em></p>
                  <p></p>

                  <h4>Results</h4>

                  <h5>Position Tracking Performance (Figure-8 Trajectory)</h5>
                  <ul>
                    <li><strong>Duration:</strong> 12 seconds</li>
                    <li><strong>RMSE X:</strong> 54.39 mm</li>
                    <li><strong>RMSE Y:</strong> 64.08 mm</li>
                    <li><strong>Total RMSE:</strong> <strong>85.92 mm</strong></li>
                  </ul>
                  <p>Controller uses only noisy EKF estimates (not true state), demonstrating realistic deployment performance.</p>

                  <img class="img-fluid d-block mx-auto" src="assets/uav/figure8/PositionTracking.png" alt="Position Tracking">
                  <p class="text-center"><em>Position tracking over time</em></p>
                  <p></p>

                  <h5>EKF State Estimation Accuracy</h5>
                  <ul>
                    <li><strong>Position RMSE:</strong> 21.4 mm (despite 30mm sensor noise)</li>
                    <li><strong>Velocity RMSE:</strong> 0.266 m/s</li>
                    <li><strong>Cable Tension RMSE:</strong> 0.0876 N (18.9% relative error)</li>
                    <li><strong>Cable Direction Error:</strong> 0.0756</li>
                  </ul>
                  <p>EKF achieves <strong>sub-sensor-noise position accuracy</strong> (21.4mm vs 30mm sensor noise) through optimal sensor fusion, providing accurate state feedback for control.</p>

                  <img class="img-fluid d-block mx-auto" src="assets/uav/figure8/CableTensionEstimation.png" alt="Cable Tension Estimation">
                  <p class="text-center"><em>Cable tension estimation: true (blue), EKF (red), nominal (green), ESO (purple)</em></p>
                  <p></p>

                  <h5>Tension Estimation: Three Complementary Methods</h5>
                  <table class="table table-sm">
                    <thead>
                      <tr>
                        <th>Method</th>
                        <th>RMSE</th>
                        <th>Purpose</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Nominal (Flatness)</td>
                        <td><strong>5.03%</strong></td>
                        <td>Feedforward control</td>
                      </tr>
                      <tr>
                        <td>ESO (Indirect)</td>
                        <td><strong>7.65%</strong></td>
                        <td>ADRC validation</td>
                      </tr>
                      <tr>
                        <td>EKF (Direct)</td>
                        <td>18.9%</td>
                        <td>Safety monitoring</td>
                      </tr>
                    </tbody>
                  </table>

                  <img class="img-fluid d-block mx-auto" src="assets/uav/figure8/ADRCValidation.png" alt="ADRC Validation">
                  <p class="text-center"><em>ADRC validation: disturbance observation and rejection</em></p>
                  <p></p>

                  <h5>Control Effort</h5>
                  <ul>
                    <li><strong>Mean thrust:</strong> 1.258 N (hover: 0.92 N)</li>
                    <li><strong>Max thrust:</strong> 1.534 N (limit: 5.0 N)</li>
                    <li><strong>Mean moment:</strong> 0.0168 N·m</li>
                    <li><strong>Max roll:</strong> 19.05°</li>
                    <li><strong>Max pitch:</strong> 17.52°</li>
                  </ul>

                  <img class="img-fluid d-block mx-auto" src="assets/uav/figure8/ControlInputs.png" alt="Control Inputs">
                  <p class="text-center"><em>Control inputs: thrust and moments over time</em></p>
                  <p></p>

                  <img class="img-fluid d-block mx-auto" src="assets/uav/figure8/Attitude.png" alt="Attitude">
                  <p class="text-center"><em>UAV attitude angles during trajectory tracking</em></p>
                  <p></p>

                  <h5>Point-to-Point Trajectory Demo</h5>
                  <video class="img-fluid d-block mx-auto" width="600" controls>
                    <source src="assets/uav/p2p/animation.webm" type="video/webm">
                    Your browser does not support the video tag.
                  </video>
                  <p class="text-center"><em>Point-to-point minimum-snap trajectory</em></p>
                  <p></p>

                  <h5>Key Achievements</h5>
                  <ul>
                    <li><strong>Hardware-ready validation:</strong> Controller operates solely on noisy sensors (σ<sub>p</sub>=30mm, σ<sub>θ</sub>=0.5°), achieving 85.92mm tracking RMSE</li>
                    <li><strong>Known-unknown decomposition:</strong> Prevents naive ADRC instability by separating nominal cable forces from disturbances</li>
                    <li><strong>EKF sensor fusion:</strong> 21.4mm position accuracy despite 30mm sensor noise, enabling accurate state feedback</li>
                    <li><strong>Multiple tension estimates:</strong> Nominal (5.03%), ESO (7.65%), EKF (18.9%) serve complementary roles</li>
                    <li><strong>System parameters:</strong> Quadrotor mass: 0.0785 kg, Payload: 0.0465 kg, Cable: 1.0 m</li>
                  </ul>

                  <ul class="list-inline">
                    <p class="item-intro text-muted"><a class="boxed" href="#"><font color="red"><b>GitHub Repository</b></font></a></p>
                    <li>Date: September 2025 — February 2026</li>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 2: Hand Teleoperation -->
    <div class="portfolio-modal modal fade" id="handteleoperation" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <h3 class="text-uppercase">Dual UR5 Hand Teleoperation System</h3>
                  <a class="boxed">ROS2</a> <a class="boxed">MediaPipe</a> <a class="boxed">Computer Vision</a> <a class="boxed">Real-time Control</a>
                  <p></p>

                  <h4>Brief Description</h4>
                  <p>An innovative teleoperation system enabling intuitive control of two UR5 robotic arms through hand gestures captured via webcam. The system leverages MediaPipe for robust hand tracking and ROS2 for reliable robot control.</p>
                  <p></p>

                  <h5>Live Demonstration</h5>
                  <video class="img-fluid d-block mx-auto" width="700" controls>
                    <source src="assets/hand_tracking/hand_tracking_demo.webm" type="video/webm">
                    Your browser does not support the video tag.
                  </video>
                  <p class="text-center"><em>Real-time hand gesture control of dual UR5 robotic arms</em></p>
                  <p></p>

                  <h4>Motivation</h4>
                  <p>Traditional robot teleoperation systems require specialized hardware and extensive training. This project aims to create an intuitive, low-cost teleoperation interface using only a standard webcam and hand gestures, making dual-arm manipulation accessible.</p>
                  <p></p>

                  <h4>Approach</h4>
                  <h5>Hand Tracking</h5>
                  <p>Leveraged MediaPipe for robust hand tracking with 21 landmarks per hand, enabling precise gesture recognition in various lighting conditions.</p>

                  <h5>Real-time Control</h5>
                  <ul>
                    <li>Implemented 50Hz control loop with exponential smoothing for stable motion</li>
                    <li>Developed intelligent mirrored hand mapping for intuitive dual arm control</li>
                    <li>Integrated pinch gesture detection for gripper control</li>
                  </ul>

                  <h5>System Integration</h5>
                  <p>Built comprehensive ROS2 system with multiple control modes (hand tracking, keyboard, automatic demo) and 3D visualization in RViz.</p>
                  <p></p>

                  <h4>Results</h4>
                  <ul>
                    <li><strong>Control frequency:</strong> 50Hz real-time control loop</li>
                    <li><strong>Latency:</strong> Low-latency hand tracking with exponential smoothing</li>
                    <li><strong>Gesture recognition:</strong> 21 landmarks per hand for precise control</li>
                    <li><strong>Dual-arm coordination:</strong> Intelligent mirrored mapping for intuitive bimanual manipulation</li>
                    <li><strong>Gripper control:</strong> Pinch gesture detection for object grasping</li>
                    <li><strong>Multiple control modes:</strong> Hand tracking, keyboard, and automatic demo modes</li>
                    <li><strong>Visualization:</strong> Real-time 3D robot visualization in RViz</li>
                    <li><strong>Robustness:</strong> Proven across different users and lighting conditions</li>
                  </ul>

                  <ul class="list-inline">
                    <p class="item-intro text-muted"><a class="boxed" href="https://github.com/Tranthanhbao198/dual_arm_teleoperation-UR5" target="_blank"><font color="red"><b>GitHub Repository</b></font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 3: Follow Me -->
    <div class="portfolio-modal modal fade" id="followme" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <h3 class="text-uppercase">Follow Me: Human-Following Mobile Robot</h3>
                  <a class="boxed">ROS2</a> <a class="boxed">Kalman Filter</a> <a class="boxed">Laser Detection</a> <a class="boxed">Mobile Robotics</a>
                  <p></p>

                  <p><strong>Duration:</strong> September 2025 — February 2026</p>
                  <p><strong>Course:</strong> Mobile Robotics, Grenoble INP</p>
                  <p></p>

                  <h4>Brief Description</h4>
                  <p>Developed a comprehensive DATMO (Detection And Tracking of Moving Objects) system for mobile robots. The system uses laser scan data to detect and track humans in real-time, enabling autonomous human-following behavior with robust performance under occlusion and sensor noise.</p>
                  <p></p>

                  <h5>Live Demonstration</h5>
                  <video class="img-fluid d-block mx-auto" width="700" controls>
                    <source src="assets/followme/followme.MOV" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <p class="text-center"><em>Mobile robot autonomously following a moving person using laser-based detection and Kalman filter tracking</em></p>
                  <p></p>

                  <h4>Motivation</h4>
                  <p>Human-following robots have applications in healthcare (assistive robotics), logistics (smart carts), and service robotics (tour guides). The challenge lies in maintaining robust tracking under occlusion and sensor noise while ensuring safe navigation in dynamic environments.</p>
                  <p></p>

                  <h4>Approach</h4>

                  <h5>1. Motion Detection</h5>
                  <p>Implemented background subtraction algorithm to detect dynamic objects in the environment:</p>
                  <ul>
                    <li><strong>Background storage:</strong> Captures static environment when robot is stationary</li>
                    <li><strong>Dynamic threshold:</strong> Identifies moving objects by comparing current scan with background</li>
                    <li><strong>Motion filtering:</strong> Distinguishes between robot's own motion and external movement</li>
                  </ul>

                  <h5>2. Laser-Based Clustering</h5>
                  <p>Developed adaptive clustering algorithm to segment laser scan data:</p>
                  <ul>
                    <li><strong>Clustering threshold:</strong> 0.05m (adaptively adjusted based on distance)</li>
                    <li><strong>Cluster properties:</strong> Computes size, center, and dynamic percentage for each cluster</li>
                    <li><strong>Advanced clustering:</strong> Handles complex scenarios with variable cluster sizes</li>
                  </ul>

                  <h5>3. Leg Detection</h5>
                  <p>Identifies human legs from clusters using geometric constraints:</p>
                  <ul>
                    <li><strong>Leg size:</strong> 0.05m - 0.25m (typical human leg width)</li>
                    <li><strong>Leg pair validation:</strong> Distance between legs: 0.1m - 0.7m</li>
                    <li><strong>Dynamic filtering:</strong> Prioritizes moving clusters to reduce false positives</li>
                  </ul>

                  <h5>4. Person Detection</h5>
                  <p>Fuses leg pairs into person detections:</p>
                  <ul>
                    <li><strong>Geometric validation:</strong> Confirms leg pair geometry matches human walking pattern</li>
                    <li><strong>Person position:</strong> Computed as midpoint between detected leg pairs</li>
                    <li><strong>Moving person selection:</strong> Identifies most likely target based on motion</li>
                  </ul>

                  <h5>5. Kalman Filter Tracking</h5>
                  <p>Implemented Kalman filter for robust state estimation:</p>
                  <ul>
                    <li><strong>State variables:</strong> Person position (x, y) in robot frame</li>
                    <li><strong>Data association:</strong> Matches detections to tracked person using nearest-neighbor</li>
                    <li><strong>Uncertainty management:</strong> Increases uncertainty during occlusion (0.5m - 1.0m)</li>
                    <li><strong>Frequency tracking:</strong> Adjusts update rate (5Hz - 25Hz) based on tracking confidence</li>
                  </ul>

                  <h5>6. Obstacle Avoidance</h5>
                  <p>Finds closest obstacle for collision-free navigation:</p>
                  <ul>
                    <li><strong>Safety margin:</strong> 0.25m robot radius + 0.3m goal offset</li>
                    <li><strong>Real-time updates:</strong> Publishes closest obstacle position for motion planner</li>
                  </ul>
                  <p></p>

                  <h4>Implementation Details</h4>
                  <ul>
                    <li><strong>Framework:</strong> ROS2 (Jazzy) with rclcpp</li>
                    <li><strong>Sensor:</strong> 2D laser scanner (1100 beams max)</li>
                    <li><strong>Visualization:</strong> RViz markers for field of view, motion, clusters, legs, persons, tracking</li>
                    <li><strong>Publishers:</strong> Detection topic, tracking topic, closest obstacle topic</li>
                    <li><strong>C++ implementation:</strong> Modular architecture with DATMO base class</li>
                  </ul>
                  <p></p>

                  <h4>Results</h4>
                  <ul>
                    <li><strong>Person detection:</strong> Successfully detects humans from laser scan leg patterns</li>
                    <li><strong>Robust tracking:</strong> Maintains person tracking through brief occlusions using Kalman filter prediction</li>
                    <li><strong>Motion detection:</strong> Distinguishes between static obstacles and moving persons</li>
                    <li><strong>Adaptive uncertainty:</strong> Uncertainty grows during occlusion (0.5m → 1.0m), decreases with good observations</li>
                    <li><strong>Real-time performance:</strong> Operates at laser scan frequency with low computational overhead</li>
                    <li><strong>Safe navigation:</strong> Obstacle detection enables collision-free following behavior</li>
                    <li><strong>Dynamic environments:</strong> Handles multiple moving objects, selects most likely target</li>
                    <li><strong>Visualization:</strong> Comprehensive RViz visualization for debugging and monitoring</li>
                  </ul>

                  <ul class="list-inline">
                    <p class="item-intro text-muted"><a class="boxed" href="#"><font color="red"><b>GitHub Repository</b></font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 4: Dual Franka -->
    <div class="portfolio-modal modal fade" id="dualfranka" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <h3 class="text-uppercase">Dual Franka Arm Pick-and-Place Using Deep RL</h3>
                  <a class="boxed">Deep RL</a> <a class="boxed">PPO</a> <a class="boxed">Isaac Gym</a> <a class="boxed">Operational Space Control</a>
                  <p></p>

                  <p><strong>Duration:</strong> October 2023 — April 2024</p>
                  <p><strong>Institution:</strong> CIR Lab, National Taiwan Normal University, Taipei, Taiwan</p>
                  <p></p>

                  <h4>Brief Description</h4>
                  <p>Research internship project training dual Franka Emika Panda robot arms to perform coordinated bottle cap removal task using Proximal Policy Optimization (PPO) in NVIDIA Isaac Gym simulator.</p>
                  <p></p>

                  <h5>Trained Policy Demonstration</h5>
                  <video class="img-fluid d-block mx-auto" width="700" controls>
                    <source src="assets/dual_franka/dual_franka_demo.webm" type="video/webm">
                    Your browser does not support the video tag.
                  </video>
                  <p class="text-center"><em>Dual Franka arms performing coordinated bottle cap removal using learned PPO policy</em></p>
                  <p></p>

                  <h4>Motivation</h4>
                  <p>Coordinated dual-arm manipulation remains challenging due to high-dimensional state-action spaces and complex contact dynamics. Deep reinforcement learning offers a data-driven approach to learn these intricate behaviors.</p>
                  <p></p>

                  <h4>Approach</h4>
                  <h5>Task Design</h5>
                  <p>Implemented custom bottle cap removal task requiring coordinated manipulation between two Franka Emika Panda arms.</p>

                  <h5>Control Architecture</h5>
                  <ul>
                    <li>Integrated PPO-based high-level policy with Operational Space Control for Cartesian manipulation</li>
                    <li>42-dimensional observation space capturing robot state and object poses</li>
                    <li>14-dimensional continuous action space for dual-arm control</li>
                  </ul>

                  <h5>Training Environment</h5>
                  <ul>
                    <li><strong>Simulator:</strong> NVIDIA Isaac Gym (GPU-accelerated physics)</li>
                    <li><strong>Parallel environments:</strong> 4096+ simultaneous simulations</li>
                    <li><strong>Multi-stage reward shaping:</strong> reach → grasp → lift → open cap</li>
                    <li><strong>Safety constraints:</strong> Collision avoidance and orientation limits</li>
                  </ul>
                  <p></p>

                  <h4>Results</h4>
                  <ul>
                    <li><strong>Policy learning:</strong> Successfully learned coordinated dual-arm manipulation policy through PPO</li>
                    <li><strong>Task completion:</strong> High success rate in bottle cap removal task</li>
                    <li><strong>Generalization:</strong> Robust performance across different initial conditions</li>
                    <li><strong>State space:</strong> 42-dimensional observation (robot state + object poses)</li>
                    <li><strong>Action space:</strong> 14-dimensional continuous control (7 DOF per arm)</li>
                    <li><strong>Training efficiency:</strong> GPU-accelerated parallel simulation enables rapid policy iteration</li>
                    <li><strong>Coordination:</strong> Learned implicit coordination between arms without explicit communication</li>
                  </ul>

                  <ul class="list-inline">
                    <p class="item-intro text-muted"><a class="boxed" href="https://github.com/Tranthanhbao198/dual_franka_ppo" target="_blank"><font color="red"><b>GitHub Repository</b></font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 5: Robothon -->
    <div class="portfolio-modal modal fade" id="robothon" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <h3 class="text-uppercase">Robothon: Robot Controller Design</h3>
                  <a class="boxed">Robot Dynamics</a> <a class="boxed">Gazebo</a> <a class="boxed">CoppeliaSim</a> <a class="boxed">Torque Control</a>
                  <p></p>

                  <p><strong>Duration:</strong> October 2021 — April 2022</p>
                  <p><strong>Institution:</strong> Technische Universität München, Germany</p>
                  <p><strong>Course:</strong> Advanced Robot Control and Learning</p>
                  <p></p>

                  <h4>Brief Description</h4>
                  <p>Comprehensive robot control course project focusing on dynamics modeling and controller implementation for the Franka Emika Panda robot arm. Worked on both simulation and real hardware demonstrations.</p>
                  <p></p>

                  <img class="img-fluid d-block mx-auto" src="https://via.placeholder.com/600x400.png?text=Add+Demo+GIF+Here" alt="">
                  <p></p>

                  <h4>Motivation</h4>
                  <p>Understanding robot dynamics and implementing low-level controllers is fundamental to robotics. This course project bridges theoretical control concepts with practical implementation on real hardware.</p>
                  <p></p>

                  <h4>Approach</h4>
                  <h5>Dynamics Derivation</h5>
                  <ul>
                    <li>Derived complete robot dynamics for Franka Emika Panda</li>
                    <li>Computed Jacobian J, Mass matrix M(q), Coriolis C(q,q̇), and Gravity g(q)</li>
                  </ul>

                  <h5>Simulation</h5>
                  <ul>
                    <li>Implemented torque-level control algorithms in Gazebo</li>
                    <li>Developed obstacle avoidance algorithms in CoppeliaSim</li>
                  </ul>

                  <h5>Hardware Validation</h5>
                  <p>Demonstrated collision handling and dynamic obstacle avoidance on real Franka Panda arm.</p>
                  <p></p>

                  <h4>Results</h4>
                  <ul>
                    <li>Successfully validated theoretical control concepts on physical hardware</li>
                    <li>Demonstrated robust collision handling and obstacle avoidance</li>
                    <li>Gained hands-on experience with torque-level robot control</li>
                  </ul>
                  <p><em>Add controller performance plots and hardware demonstration videos here</em></p>

                  <ul class="list-inline">
                    <p class="item-intro text-muted"><a class="boxed" href="#"><font color="red"><b>GitHub Repository</b></font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 6: ZV-ADRC -->
    <div class="portfolio-modal modal fade" id="zvadrc" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <h3 class="text-uppercase">ZV Shaper – ADRC Crane Control</h3>
                  <a class="boxed">ADRC</a> <a class="boxed">Input Shaping</a> <a class="boxed">Control Theory</a> <a class="boxed">Published Research</a>
                  <p></p>

                  <p><strong>Duration:</strong> May 2021 — September 2023</p>
                  <p><strong>Institution:</strong> Rockwell Automation Lab, Hanoi University of Science and Technology</p>
                  <p></p>

                  <h4>Brief Description</h4>
                  <p>Research on combining Zero-Vibration (ZV) input-shaping technique with Active Disturbance Rejection Control (ADRC) to improve crane system performance while respecting actuator constraints. Published in Mechatronic Systems and Control (MCA) journal.</p>
                  <p></p>

                  <img class="img-fluid d-block mx-auto" src="https://via.placeholder.com/600x400.png?text=Add+Demo+GIF+Here" alt="">
                  <p></p>

                  <h4>Motivation</h4>
                  <p>Crane systems suffer from payload oscillation during operation, reducing efficiency and safety. While ADRC provides robust disturbance rejection and ZV input shaping suppresses vibration, combining them while respecting actuator constraints presents a novel challenge.</p>
                  <p></p>

                  <h4>Approach</h4>
                  <h5>Controller Design</h5>
                  <ul>
                    <li>Combined Zero-Vibration (ZV) input-shaping technique with Active Disturbance Rejection Control (ADRC)</li>
                    <li>Enhanced controllers to respect input-signal constraints and actuator limits</li>
                    <li>Developed constrained control strategy for practical implementation</li>
                  </ul>

                  <h5>Validation</h5>
                  <ul>
                    <li>MATLAB/Simulink simulation studies</li>
                    <li>Hardware implementation and testing</li>
                    <li>Performance comparison with PID and Sliding Mode Control</li>
                  </ul>
                  <p></p>

                  <h4>Results</h4>
                  <ul>
                    <li>Demonstrated superior performance compared to traditional PID approaches</li>
                    <li>Successfully validated approach on physical hardware</li>
                    <li>Published research findings in peer-reviewed journal</li>
                  </ul>

                  <h5>Publication</h5>
                  <p><strong>Tran Thanh Bao</strong>, Nguyen Minh Duc, M. c Dng, and T. H. Do, "ZV Shaper – ADRC Combination Control for Crane System with Constrained Control Signal", <em>Mechatronic Systems and Control (MCA)</em>, Vol. 4, No. 3, pp. 32–38, December 2023.</p>

                  <ul class="list-inline">
                    <p class="item-intro text-muted"><a class="boxed" href="#"><font color="red"><b>Read Paper</b></font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.2/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>

    <script>
      (function($) {
        "use strict";

        // Smooth scrolling using jQuery easing
        $('a.js-scroll-trigger[href*="#"]:not([href="#"])').click(function() {
          if (location.pathname.replace(/^\//, '') == this.pathname.replace(/^\//, '') && location.hostname == this.hostname) {
            var target = $(this.hash);
            target = target.length ? target : $('[name=' + this.hash.slice(1) + ']');
            if (target.length) {
              $('html, body').animate({
                scrollTop: (target.offset().top - 54)
              }, 1000, "easeInOutExpo");
              return false;
            }
          }
        });

        // Closes responsive menu when a scroll trigger link is clicked
        $('.js-scroll-trigger').click(function() {
          $('.navbar-collapse').collapse('hide');
        });

        // Activate scrollspy to add active class to navbar items on scroll
        $('body').scrollspy({
          target: '#mainNav',
          offset: 56
        });

        // Collapse Navbar
        var navbarCollapse = function() {
          if ($("#mainNav").offset().top > 100) {
            $("#mainNav").addClass("navbar-shrink");
          } else {
            $("#mainNav").removeClass("navbar-shrink");
          }
        };
        // Collapse now if page is not at top
        navbarCollapse();
        // Collapse the navbar when page is scrolled
        $(window).scroll(navbarCollapse);

      })(jQuery);
    </script>

</body>
</html>
